%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% preamble
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass{book}
\usepackage{ctex}
\usepackage{indentfirst} 
% code block
\usepackage{listings}
\usepackage{xcolor}
\lstset{ 
 breaklines,
 numbers=left,                                       
 numberstyle=\tiny\color{black},                   
 frame=none,                                          
 backgroundcolor=\color[RGB]{224,224,244},          
 keywordstyle=\color[RGB]{40,40,255},              
 numberstyle=\footnotesize\color{darkgray},           
 commentstyle=\it\color[RGB]{0,96,96},               
 stringstyle=\rmfamily\color[RGB]{128,0,0},   
 showstringspaces=false,                             
 language=Python                               
}
% math
\usepackage{amsmath}
\usepackage{amsfonts}
% link
\usepackage[colorlinks,linkcolor=green]{hyperref}
\usepackage{graphicx} 
% 纸张设置
\usepackage{geometry}
\geometry{a4paper} % 纸张大小
\geometry{left=2cm} % 左边距
\geometry{right=2cm} % 右边距
\geometry{top=1cm} % 顶部边距
\geometry{bottom=0.5cm} % 底部边距
\geometry{hcentering}  % 水平居中
\geometry{vcentering} % 垂直居中

\begin{document}
\tableofcontents
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 参考
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{参考}
\section{Course}
\noindent MOOC\\
\url{https://www.icourse163.org/course/ZJU-1003377027}\\
\url{https://www.icourse163.org/course/CUG-1003556007}\\
\url{https://www.icourse163.org/course/BIT-1001872001}\\
\url{https://www.icourse163.org/course/FJNU-1205696811}\\
\url{https://www.icourse163.org/course/ZJUT-1002694018}\\
David Silver(UCL)\\
\url{http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html}\\
Emma Brunskill(Stanford)\\
\url{https://onlinehub.stanford.edu/cs234}\\
Katerina\&Russ(CMU)\\
\url{https://katefvision.github.io/}\\
Sergey Levine(UC Berkeley)\\
\url{http://rail.eecs.berkeley.edu/deeprlcourse/}\\
李宏毅\\
\url{https://www.bilibili.com/video/av24724071?from=search&seid=8124301089510362159}\\
莫烦\\
\url{https://www.bilibili.com/video/av16921335}
\section{Book}
\noindent 郭宪\&方勇纯\\
《深入浅出强化学习:原理入门》\\
李玉喜\\
\url{https://arxiv.org/pdf/1810.06339.pdf}\\
Richard S. Sutton and Andrew G. Barto\\
\url{http://www.incompleteideas.net/book/RLbook2018.pdf}\\
Code for Sutton's Book\\
\url{https://github.com/ShangtongZhang/reinforcement-learning-an-introduction}\\
Csaba Szepesvári\\
\url{https://sites.ualberta.ca/~szepesva/RLBook.html}
\section{Web}
\noindent \url{https://spinningup.openai.com/en/latest/index.html#}\\
\url{https://medium.com/@yuxili/resources-for-deep-reinforcement-learning-a5fdf2dc730f}\\
\url{https://zhuanlan.zhihu.com/sharerl}
\section{Environment}
\noindent \url{gym.openai.com}\\
\url{http://vizdoom.cs.put.edu.pl/}
\section{Other}
\noindent 大红豆与小薏米\\
\url{https://www.bilibili.com/video/av49055416}\\
\url{https://www.bilibili.com/video/av48758533}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 递归
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{递归}
\section{概念}
程序调用自身的编程技巧称为递归，递归问题的关键是写出递推关系式和边界条件，
递推关系式的左边是函数名，递归关系的右边是函数返回值，边界条件是函数的终
止条件。
\section{示例}
计算斐波那契数列的第n项：1，1，2，3，5，8，...
\begin{lstlisting} 
def fib(n): # 递归表达式的左边
    if n == 1: # 边界条件
        return 1
    elif n == 2: # 边界条件
        return 1
    else:
        return fib(n-1)+fib(n-2) # 递归表达式的右边
\end{lstlisting}   
\section{衍生}
计算斐波那契数列的前$n$项和
分析：如果知道前$n-1$项的和为$Sum(n-1)$，则$Sum(n)=fib(n)+Sum(n-1)$
\begin{lstlisting} 
# 计算fib(n)
def fib(n): 
    if n == 1: 
        return 1
    elif n == 2: 
        return 1
    else:
        return fib(n-1) + fib(n-2) 
# 计算Sum(n)
def Sum(n):
    if n == 1:
        return 1
    return fib(n) + Sum(n-1)
\end{lstlisting}   
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 动态规划
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{动态规划}
\section{概念}
\noindent 重叠子问题\\
状态转移方程
\section{示例}
\begin{enumerate}
\item 斐波那契数列\\
给定一组斐波那契数列：1，1，2，3，5，8，...;其状态转移方程为：$F(n)=F(n-1) + F(n-2) (n>2)$
\begin{lstlisting}
memo = []
def fib(n):
    memo.append(1)
    memo.append(1)
    if n==1 or n==2:
        return 1
    elif n>2:
        for i in range(n-2):
            memo.append(memo[i] + memo[i+1]) 
        return memo[-1]
\end{lstlisting}
\item 最大工作报酬\\
\begin{figure}[!htbp]
\centering
\caption{最大工作报酬}
\includegraphics[width=6cm]{./res/最大工作报酬.pdf}
\end{figure}

如果只有1工作可选，则最大报酬为$dp(1)=5$\\
如果只有1,2两个工作可选，则最大报酬为$dp(2)=5$\\
如果可选工作为1,2,3则最大报酬为$dp(3)=8$\\
如果可选工作为1,2,3,4则最大报酬为$dp(4)=max(4+dp(1),dp(3))$\\
如果可选工作为1,2,3,4,5则最大报酬为$dp(5)=max(6+none, dp(4))$\\
如果可选工作为1,2,3,4,5,6则最大报酬为$dp(6)=max(3+dp(2), dp(5))$\\

\setlength{\parindent}{2em} 从i份工作可选过渡到i+1份工作可选的过程中，第i+1份工作如果做的话则报酬为第i+1份工作的报酬加上dp(最大可做工作的序号)，如果第i+1份工作不做的话则最大报酬为dp(i)；所以状态转移方程为：\\
dp(i+1)=max(第i+1份工作的报酬+dp(最大可做工作的序号),dp(i))

\item 最大不相邻元素子序列和
\begin{figure}[!htbp]
\caption{最大不相邻元素子序列和}
\includegraphics[width=6cm]{./res/最大子序列和.pdf}
\centering
\end{figure}

\noindent 如果只有第0号元素，dp(1)=1\\
如果只有第0,1号元素，dp(2)=2\\
如果只有第0,1,2号元素，dp(3)=max(arr[3]+dp(1),dp(2)\\

\setlength{\parindent}{2em} 从一个长为i的序列扩展到长为i+1的序列的状态转移方程为：\\
$$dp(i+1)=max(dp(i),dp(i-1)+arr[i+1])$$
\begin{lstlisting} 
def dp(arr):
    if len(arr)==1:
        return arr[0]
    if len(arr)==2:
        return max(arr[0], arr[1])
    if len(arr)>=3:
        sumMemo=[arr[0], max(arr[0], arr[1])]
        for i in range(2, len(arr)):
            n = max(sumMemo[i-1],sumMemo[i-2]+arr[i])
            sumMemo.append(n)
        return sumMemo[-1]
arr=[1,2,4,1,7,8,3]
dp(arr) # 15
\end{lstlisting}   
\item 定和组合\\
从arr=[3,34,4,12,5,2]组合出所有S=9的子序列\\
\begin{figure}[!htbp]
\caption{定和组合}
\includegraphics[width=14cm]{./res/定和组合.pdf}
\centering
\end{figure}

\begin{figure}[!htbp]
\begin{lstlisting}
import numpy as np 
def dp(arr, S): 
    memo = dict() 
    for i in range(len(arr)): 
        memo[i] = {'choose':[], 'not choose':[]} 
        present = arr[i] 
        if i == 0: 
            if S-present>0: 
                memo[i]['choose'].append(S-present) 
            memo[i]['not choose'].append(S) 
        else: 
            if memo[i-1]['choose']: 
                for j in memo[i-1]['choose']: 
                    if j-present==0: 
                        print('YES') 
                    if j-present>0: 
                        memo[i]['choose'].append(j-present) 
                    memo[i]['not choose'].append(j) 
            if memo[i-1]['not choose']: 
                for k in memo[i-1]['not choose']: 
                    if k-present==0: 
                        print('YES')  
                    if k-present>0: 
                        memo[i]['choose'].append(k-present)  
                    memo[i]['not choose'].append(k)   
    print(memo) 
arr=[3,34,4,12,5,2] 
dp(arr, 9)
\end{lstlisting} 
\end{figure}
\end{enumerate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 强化学习
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{强化学习}
强化学习中智能体通过和环境互动不断地试错(trial and error)来得到环境的
反馈，根据反馈来进一步纠正自己的行动策略；具体来说，智能体根据状态$S$来采取动作
$A$，得到环境返回的奖励$R$，和下一时刻的状态$S'$，不断的重复这一过程来
采集环境反馈回来的数据逐步优化策略$\pi$。\\
\section{基本概念}
\begin{enumerate}
	\item 智能体
    \item 环境\\
    分为部分可观测和全部可观测
	\item 状态
	\item 策略\\
    策略是从状态到动作的映射，分为Deterministic和Stochastic两种\\
    值函数越准确，策略越差的现象称为策略退化\\
    Stochastic策略函数$\pi$:S x A → [0,1]，其中$\pi$(s, a)的值表示在状态s下采取动作a的概率
	\item 动作
	\item 奖励
	\item Trajectory
    \item Discount Factor $\gamma$   
	\item 马尔可夫过程\\
	当前时刻的状态仅与上一时刻的状态有关
	\item 马尔可夫链
	$$P\left(s_{t+1} | s_{t}\right)=P\left(s_{t+1} | s_{t}, s_{t-1}, \ldots s_{1}, s_{0}\right)$$
	\item 马尔可夫决策过程\\
    强化学习研究的是马尔可夫决策过程(MDP)，由一个五元组$(S,A,P,R,\gamma)$来表示
	\item 累计回报
	$$G_{t}=R_{t+1}+\lambda R_{t+2}+\ldots=\sum_{k=0}^{\infty} \lambda^{k} R_{t+k+1}$$\\
	递归形式:$$G_{t}=R_{t+1}+\gamma G_{t+1}$$
	\item V值\\
	$V^{\pi} : S \rightarrow \mathbb{R}$是累积回报$G_{t}$的期望，$V^{\pi}$是价值函数，从状态s开始，根据$\pi$来采取行动：
	$$V^{\pi}(s)=E_{\pi}\left[R\left(s_{0}, a_{0}\right)+\gamma R\left(s_{1}, a_{1}\right)+\gamma^{2} R\left(s_{2}, a_{2}\right)+\ldots | s_{0}=s\right]$$
	\item Q值\\
    $Q^{\pi} : S \times A \rightarrow \mathbb{R}$，$Q$函数从一个给定的状态$s$开始，首先采取一个指定的动作$a$，然后根据策略$\pi$采取后续动作给出累计回报$G_{t}$的期望：
    $$Q^{\pi}(s, a)=E_{\pi}\left[R\left(s_{0}, a_{0}\right)+\gamma R\left(s_{1}, a_{1}\right)+\gamma^{2} R\left(s_{2}, a_{2}\right)+\ldots | s_{0}=s, a_{0}=a, \forall t>0 \quad a_{t}=\pi\left(s_{t}\right)\right]$$
	\item 最优价值函数
	$$V^{*}(s)=\underset{\pi} {\max} V^{\pi}(s)$$
	\item 最优$Q$函数
	$$Q^{*}(s, a)=\max _{\pi} Q^{\pi}(s, a)$$
	\item 贝尔曼方程
	\item Exploration-Exploitation Dilemma
	\item Value-Based
	\item Policy Gradient
	\item Inverse Reinforcement Learning
	\item Imitation Learning
	\item On-Policy
	\item Off-Policy
	\item 时间差分(TD)和蒙特卡洛(MC)
	\item 动态规划PredictionControl
	\item
	\item
\end{enumerate}
\section{最优策略定理}
基于值函数的方法\\
收敛慢——需要对 V or Q和 $\pi$ 交替优化,
方差小（值函数基于过去的训练样本得到，置信度较高，而策略基于值函数）\\
策略梯度方法\\
收敛快——直接对 $\pi$ 进行优化，方差大\\
基于策略的算法算法每一轮都使用当前的策略进行采样，这样并不能保证所有的状态都被访问到，更不能保证均匀地访问到所有的状态，在这种情况下，一般很难有 global optimality。（考虑有些可能很『好』的区域根本没有被策略访问到）对比基于值函数的方法（比如 Q-learning、certainty-equivalence），它们只需要零散的 transition，这样就能要求在足够多的样本上把任意状态访问足够多次，从而有相应的 contraction。

作者：张楚珩
链接：https://zhuanlan.zhihu.com/p/78919869
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。
\section{收敛性对比}

\section{强化学习的特点}
\begin{itemize}
	\item no supervisor
	\item reward is sparse and delayed
	\item not idd
	\item non-stationary distribution
\end{itemize}
\section{分类}
\begin{enumerate}
\item 根据基于价值或者基于策略可分为:\\
Policy-based\\
Policy Gradient\\

Value-based\\
Q-learning\\
Sarsa\\

\item 根据环境是否已知分为:\\
Model-Based RL\\
Model-Free RL\\
MDP过程由一个五元组来确定<S, A, P, R, $\gamma$>，Policy是一个从S到
A的映射，R常常是已知的，所以环境是否已知是指状态转移矩阵P是否已知

\item 更新方式
单步更新
回合更新

\item 是否在线
on-policy
off-policy
\end{enumerate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Value-based
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Value-based}
\section{Q-learning}
\subsection{Paper}
\subsubsection{Link}
\noindent 1989年Watkins提出了Q-Learning方法，1992年和Dayan证明了其收敛性\\
\url{https://link.springer.com/content/pdf/10.1007/BF00992698.pdf}
\subsubsection{Formula}
$Q(s, a) =Q(s, a)+\alpha\left[r+\gamma max_{a^{\prime}} Q\left(s^{\prime}, a^{\prime}\right)-Q(s, a)\right]$
\subsection{Code}
\begin{lstlisting}
"""
A simple example for Reinforcement Learning using table lookup Q-learning method.
An agent "o" is on the left of a 1 dimensional world, the treasure is on the rightmost location.
Run this program and to see how the agent will improve its strategy of finding the treasure.
View more on my tutorial page: https://morvanzhou.github.io/tutorials/
"""

import numpy as np
import pandas as pd
import time

np.random.seed(2)  # reproducible


N_STATES = 6   # the length of the 1 dimensional world
ACTIONS = ['left', 'right']     # available actions
EPSILON = 0.9   # greedy police
ALPHA = 0.1     # learning rate
GAMMA = 0.9    # discount factor
MAX_EPISODES = 13   # maximum episodes
FRESH_TIME = 0.3    # fresh time for one move


def build_q_table(n_states, actions):
    table = pd.DataFrame(
        np.zeros((n_states, len(actions))),     # q_table initial values
        columns=actions,    # actions's name
    )
    # print(table)    # show table
    return table


def choose_action(state, q_table):
    # This is how to choose an action
    state_actions = q_table.iloc[state, :]
    if (np.random.uniform() > EPSILON) or ((state_actions == 0).all()):  # act non-greedy or state-action have no value
        action_name = np.random.choice(ACTIONS)
    else:   # act greedy
        action_name = state_actions.idxmax()    # replace argmax to idxmax as argmax means a different function in newer version of pandas
    return action_name


def get_env_feedback(S, A):
    # This is how agent will interact with the environment
    if A == 'right':    # move right
        if S == N_STATES - 2:   # terminate
            S_ = 'terminal'
            R = 1
        else:
            S_ = S + 1
            R = 0
    else:   # move left
        R = 0
        if S == 0:
            S_ = S  # reach the wall
        else:
            S_ = S - 1
    return S_, R


def update_env(S, episode, step_counter):
    # This is how environment be updated
    env_list = ['-']*(N_STATES-1) + ['T']   # '---------T' our environment
    if S == 'terminal':
        interaction = 'Episode %s: total_steps = %s' % (episode+1, step_counter)
        print('\r{}'.format(interaction), end='')
        time.sleep(2)
        print('\r                                ', end='')
    else:
        env_list[S] = 'o'
        interaction = ''.join(env_list)
        print('\r{}'.format(interaction), end='')
        time.sleep(FRESH_TIME)


def rl():
    # main part of RL loop
    q_table = build_q_table(N_STATES, ACTIONS)
    for episode in range(MAX_EPISODES):
        step_counter = 0
        S = 0
        is_terminated = False
        update_env(S, episode, step_counter)
        while not is_terminated:

            A = choose_action(S, q_table)
            S_, R = get_env_feedback(S, A)  # take action & get next state and reward
            q_predict = q_table.loc[S, A]
            if S_ != 'terminal':
                q_target = R + GAMMA * q_table.iloc[S_, :].max()   # next state is not terminal
            else:
                q_target = R     # next state is terminal
                is_terminated = True    # terminate this episode

            q_table.loc[S, A] += ALPHA * (q_target - q_predict)  # update
            S = S_  # move to next state

            update_env(S, episode, step_counter+1)
            step_counter += 1
    return q_table


if __name__ == "__main__":
    q_table = rl()
    print('\r\nQ-table:\n')
    print(q_table)
\end{lstlisting}
\subsection{收敛性证明}
%-----------------------------------------------------------
\section{Sarsa}
\subsection{Paper}
\subsection{Code}
%-----------------------------------------------------------
\section{DQN}
\url{https://www.youtube.com/watch?v=wrBUkpiRvCA}\\
\subsection{Paper}
\begin{figure}[!htbp]
\centering
\includegraphics[width=6cm]{./res/DQN.jpg}
\caption{map state to action}
\end{figure}
\subsection{Code}
\begin{lstlisting}
###################################################
# Import necessary dependencies
###################################################
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import gym
###################################################
# Hyper parameters
###################################################
BATCH_SIZE = 32
LR = 0.01                   # learning rate
EPSILON = 0.9               # greedy policy
GAMMA = 0.9                 # reward discount
TARGET_REPLACE_ITER = 100   # target update frequency
MEMORY_CAPACITY = 2000
###################################################
# Creat environment
###################################################
env = gym.make('CartPole-v0')
env = env.unwrapped
N_ACTIONS = env.action_space.n
N_STATES = env.observation_space.shape[0]
ENV_A_SHAPE = 0 if isinstance(env.action_space.sample(), int) else env.action_space.sample().shape     
# to confirm the shape

class Net(nn.Module):
    def __init__(self, ): 
        super(Net, self).__init__()
        self.fc1 = nn.Linear(N_STATES, 50)
        self.fc1.weight.data.normal_(0, 0.1)   # initialization
        self.out = nn.Linear(50, N_ACTIONS)
        self.out.weight.data.normal_(0, 0.1)   # initialization

    def forward(self, x):
        x = self.fc1(x)
        x = F.relu(x)
        actions_value = self.out(x)
        return actions_value


class DQN(object):
    def __init__(self):
        self.eval_net, self.target_net = Net(), Net() # Fixed Q-targets
        self.learn_step_counter = 0                                     # for target updating
        self.memory_counter = 0                                         # for storing memory
        self.memory = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 2))     # initialize memory
        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR)
        self.loss_func = nn.MSELoss()

    def choose_action(self, x):
        x = torch.unsqueeze(torch.FloatTensor(x), 0)
        # input only one sample
        if np.random.uniform() < EPSILON:   # greedy
            actions_value = self.eval_net.forward(x)
            action = torch.max(actions_value, 1)[1].data.numpy()
            action = action[0] if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)  # return the argmax index
        else:   # random
            action = np.random.randint(0, N_ACTIONS)
            action = action if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)
        return action

    def store_transition(self, s, a, r, s_):
        transition = np.hstack((s, [a, r], s_))
        # replace the old memory with new memory
        index = self.memory_counter % MEMORY_CAPACITY
        self.memory[index, :] = transition
        self.memory_counter += 1

    def learn(self):
        # target parameter update
        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:
            self.target_net.load_state_dict(self.eval_net.state_dict())
        self.learn_step_counter += 1

        # sample batch transitions
        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)
        b_memory = self.memory[sample_index, :]
        b_s = torch.FloatTensor(b_memory[:, :N_STATES])
        b_a = torch.LongTensor(b_memory[:, N_STATES:N_STATES+1].astype(int))
        b_r = torch.FloatTensor(b_memory[:, N_STATES+1:N_STATES+2])
        b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:])

        # q_eval w.r.t the action in experience
        q_eval = self.eval_net(b_s).gather(1, b_a)  # shape (batch, 1)
        q_next = self.target_net(b_s_).detach()     # detach from graph, don't backpropagate
        q_target = b_r + GAMMA * q_next.max(1)[0].view(BATCH_SIZE, 1)   # shape (batch, 1)
        loss = self.loss_func(q_eval, q_target)

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

dqn = DQN()

print('\nCollecting experience...')
for i_episode in range(400):
    s = env.reset()
    ep_r = 0
    while True:
        env.render()
        a = dqn.choose_action(s)

        # take action
        s_, r, done, info = env.step(a)

        # modify the reward
        x, x_dot, theta, theta_dot = s_
        r1 = (env.x_threshold - abs(x)) / env.x_threshold - 0.8
        r2 = (env.theta_threshold_radians - abs(theta)) / env.theta_threshold_radians - 0.5
        r = r1 + r2

        dqn.store_transition(s, a, r, s_)

        ep_r += r
        if dqn.memory_counter > MEMORY_CAPACITY:
            dqn.learn()
            if done:
                print('Ep: ', i_episode,
                      '| Ep_r: ', round(ep_r, 2))

        if done:
            break
        s = s_
\end{lstlisting}
\subsection{衍生}
\subsubsection{Double DQN}
\subsubsection{Dueling Network}
\subsubsection{Prioritized Replay}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Policy-based
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Policy-based}
\section{Policy Gradient}
\subsection{Paper}
\subsection{Code}
\noindent \url{https://github.com/geektutu/tensorflow-tutorial-samples/tree/master/gym/CartPole-v0-policy-gradient}
%-----------------------------------------------------------
\section{DPG}
\subsection{Paper}
\subsection{Code}
%-----------------------------------------------------------
\section{DDPG}
\subsection{Paper}
\subsection{Code}
%-----------------------------------------------------------
\section{TRPO}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Actor-Critic
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Actor-Critic} 
\noindent AC\\
\url{https://homes.cs.washington.edu/~todorov/courses/amath579/reading/PolicyGradient.pdf}\\
A2C\\
\url{https://openai.com/blog/baselines-acktr-a2c/}\\
A3C\\
\url{https://arxiv.org/abs/1602.01783}
\section{AC}
\subsection{Paper}
\subsection{Code}
\begin{lstlisting}
import numpy as np
import tensorflow as tf
import gym
# tensorflow=1.14.0; gym=0.14.0; numpy=1.16.4 

# 超参数
OUTPUT_GRAPH = False
MAX_EPISODE = 3000
DISPLAY_REWARD_THRESHOLD = 200  # 刷新阈值
MAX_EP_STEPS = 1000             #最大迭代次数
RENDER = False  # 渲染开关
GAMMA = 0.9     # 衰变值
LR_A = 0.001    # Actor学习率
LR_C = 0.01     # Critic学习率

env = gym.make('CartPole-v0')
env.seed(1)  
env = env.unwrapped

N_F = env.observation_space.shape[0] # 状态空间
N_A = env.action_space.n             # 动作空间


class Actor(object):
    def __init__(self, sess, n_features, n_actions, lr=0.001):
        self.sess = sess

        self.s = tf.placeholder(tf.float32, [1, n_features], "state")
        self.a = tf.placeholder(tf.int32, None, "act")
        self.td_error = tf.placeholder(tf.float32, None, "td_error")  # TD_error

        with tf.variable_scope('Actor'):
            l1 = tf.layers.dense(
                inputs=self.s,
                units=20,    # number of hidden units
                activation=tf.nn.relu,
                kernel_initializer=tf.random_normal_initializer(0., .1),    # weights
                bias_initializer=tf.constant_initializer(0.1),  # biases
                name='l1'
            )

            self.acts_prob = tf.layers.dense(
                inputs=l1,
                units=n_actions,    # output units
                activation=tf.nn.softmax,   # get action probabilities
                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights
                bias_initializer=tf.constant_initializer(0.1),  # biases
                name='acts_prob'
            )

        with tf.variable_scope('exp_v'):
            log_prob = tf.log(self.acts_prob[0, self.a])
            self.exp_v = tf.reduce_mean(log_prob * self.td_error)  # advantage (TD_error) guided loss

        with tf.variable_scope('train'):
            self.train_op = tf.train.AdamOptimizer(lr).minimize(-self.exp_v)  # minimize(-exp_v) = maximize(exp_v)

    def learn(self, s, a, td):
        s = s[np.newaxis, :]
        feed_dict = {self.s: s, self.a: a, self.td_error: td}
        _, exp_v = self.sess.run([self.train_op, self.exp_v], feed_dict)
        return exp_v

    def choose_action(self, s):
        s = s[np.newaxis, :]
        probs = self.sess.run(self.acts_prob, {self.s: s})   # 获取所有操作的概率
        return np.random.choice(np.arange(probs.shape[1]), p=probs.ravel())   # return a int


class Critic(object):
    def __init__(self, sess, n_features, lr=0.01):
        self.sess = sess

        self.s = tf.placeholder(tf.float32, [1, n_features], "state")
        self.v_ = tf.placeholder(tf.float32, [1, 1], "v_next")
        self.r = tf.placeholder(tf.float32, None, 'r')

        with tf.variable_scope('Critic'):
            l1 = tf.layers.dense(
                inputs=self.s,
                units=20,  # number of hidden units
                activation=tf.nn.relu,  # None
                # have to be linear to make sure the convergence of actor.
                # But linear approximator seems hardly learns the correct Q.
                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights
                bias_initializer=tf.constant_initializer(0.1),  # biases
                name='l1'
            )

            self.v = tf.layers.dense(
                inputs=l1,
                units=1,  # output units
                activation=None,
                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights
                bias_initializer=tf.constant_initializer(0.1),  # biases
                name='V'
            )

        with tf.variable_scope('squared_TD_error'):
            self.td_error = self.r + GAMMA * self.v_ - self.v
            self.loss = tf.square(self.td_error)    # TD_error = (r+gamma*V_next) - V_eval
        with tf.variable_scope('train'):
            self.train_op = tf.train.AdamOptimizer(lr).minimize(self.loss)

    def learn(self, s, r, s_):
        s, s_ = s[np.newaxis, :], s_[np.newaxis, :]

        v_ = self.sess.run(self.v, {self.s: s_})
        td_error, _ = self.sess.run([self.td_error, self.train_op],
                                          {self.s: s, self.v_: v_, self.r: r})
        return td_error
sess = tf.Session()
actor = Actor(sess, n_features=N_F, n_actions=N_A, lr=LR_A)  # 初始化Actor
critic = Critic(sess, n_features=N_F, lr=LR_C)     # 初始化Critic
sess.run(tf.global_variables_initializer())        # 初始化参数
if OUTPUT_GRAPH:
    tf.summary.FileWriter("logs/", sess.graph)        # 输出日志
# 开始迭代过程 对应伪代码部分
for i_episode in range(MAX_EPISODE):
    s = env.reset() # 环境初始化
    t = 0
    track_r = []    # 每回合的所有奖励
    while True:
        if RENDER: env.render()
        a = actor.choose_action(s)       # Actor选取动作
        s_, r, done, info = env.step(a)   # 环境反馈
        if done: r = -20    # 回合结束的惩罚
        track_r.append(r)  # 记录回报值r
        td_error = critic.learn(s, r, s_)  # Critic 学习
        actor.learn(s, a, td_error)        # Actor 学习
        s = s_
        t += 1

        if done or t >= MAX_EP_STEPS:
            # 回合结束, 打印回合累积奖励
            ep_rs_sum = sum(track_r)
            if 'running_reward' not in globals():
                running_reward = ep_rs_sum
            else:
                running_reward = running_reward * 0.95 + ep_rs_sum * 0.05
            if running_reward > DISPLAY_REWARD_THRESHOLD: RENDER = True  # rendering
            print("episode:", i_episode, "  reward:", int(running_reward))
            break
\end{lstlisting}
%-----------------------------------------------------------
\section{A2C}
\subsection{Paper}
\subsection{Code}
%-----------------------------------------------------------
\section{A3C}
\subsection{Paper}
\subsection{Code}
\chapter{Imitation Learning}
\section{IRL}
\section{Behavior Cloning}
\end{document}
